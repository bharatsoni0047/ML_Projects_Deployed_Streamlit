{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatsoni0047/ml_deploy/blob/master/Default_Credit_Card_Deployment_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Credit Card Default Prediction**\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Type**    - Classification\n",
        "\n",
        "**Contribution**    - Team\n",
        "\n",
        "**Team Member 1**- Bharat Kumar Soni\n",
        "\n",
        "**Team Member 2**- Abhilasha\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to train various supervised learning algorithms to predict the client’s behavior in paying off the credit card balance. In classification problems, an imbalanced dataset is also crucial to enhance the performance of the model, so different resampling techniques were also used to balance the dataset. We first investigated the datasets by using exploratory data analysis techniques, including data normalization.\n",
        "\n",
        " We started with the logistic regression model, then compared the results with traditional machine learning-based models. Then K-means SMOTE resampling method on Taiwan client’s credit dataset.\n",
        "\n",
        "In the end, the proposed method has also been deployed on the web to assist the different stakeholders. Therefore, when the financial institution considers issuing the client a credit card, the institution needs to check the payment history of that person because the decision on whether to pay on duly or owe the bill on a specific month usually relates to the previous payment history.\n",
        "\n",
        "For instance, if a person owes numerous bills already, he or she is likely to delay the payment of the current month unless this person gets a windfall so that the total arrears can be paid off. Besides the payment history, it is also imperative to look at the applicants’ credit limit of their current credit cards. This is a result of a virtuous circle: people who pay on duly tend to have better credit scores, so the banks prefer to increase these people’s credit lines by taking less risk. As a result, if a potential client already has a credit card with a high credit limit line, this person is unlikely to fail to pay the full amount owed in the future.\n",
        "\n",
        "Although the financial institution often collects clients’ personal information such as age, educational level, and marital status when people apply for credit cards, this information also affects the default behavior."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/bharatsoni0047/Credit-Card-Default-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project is aimed at predicting the case of customers default payments in Taiwan.**\n",
        "\n",
        "From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients.\n",
        "\n",
        " We can use the K-S chart to evaluate which customers will default on their credit card payments"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrjSC-F8HJv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Know Your Data-**"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/Supervised/Classification/Default of credit card clients.xls', header=1)"
      ],
      "metadata": {
        "id": "N2N3wmgVZLXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()   #first 5 rows"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()   #last 5 rows"
      ],
      "metadata": {
        "id": "_O6oaZYWapFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that we have around 30000 rows and 25 columns in our dataset."
      ],
      "metadata": {
        "id": "pHOiIDcwa4Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since there are too many columns in the dataframe, we are not able to see all of them.\n",
        "# we can remedy this using set_option function.\n",
        "pd.set_option('display.max_columns', None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tQOaz03YbF-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting descriptive statistics of the data.\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "4XQec2UzdZ3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Countd\n",
        "duplicates = df.duplicated()\n",
        "\n",
        "# Count the number of duplicates\n",
        "duplicate_count = duplicates.sum()\n",
        "# Print the number of duplicates\n",
        "print(f\"Number of duplicates: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no duplicate values in our dataset"
      ],
      "metadata": {
        "id": "QqujUxsAciBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we dont have any null values in our dataset"
      ],
      "metadata": {
        "id": "KHtSzcn8cnLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kf8Vauf4WtfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables and predict the case of customers default payments in Taiwan."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing-**\n"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ID: ID of each client (unique identifier)\n",
        "2. LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
        "3. SEX: Gender (1=male, 2=female)\n",
        "4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
        "6. AGE: Age in years\n",
        "7. PAY_0: Repayment status in September, 2005 (-2 = Unused,-1=pay duly,0=Revolving Credit, 1=payment delay for one month, 2=payment delay for two months,8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "8. PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "9.PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "10.PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "11.PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "12.PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "13.BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "14.BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "15.BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "16.BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "17.BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "18.BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "19.PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "20.PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "21.PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "22.PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "23.PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "24.PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "25.default.payment.next.month: Default payment (1=yes, 0=no)\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's rename the columns for better understanding.\n",
        "df.rename(columns={'PAY_0':'REPAY_STATUS_SEPT','PAY_2':'REPAY_STATUS_AUG','PAY_3':\n",
        "                   'REPAY_STATUS_JUL','PAY_4':'REPAY_STATUS_JUN','PAY_5':'REPAY_STATUS_MAY','PAY_6':'REPAY_STATUS_APR'},inplace=True)\n",
        "\n",
        "df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG',\n",
        "                   'BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "\n",
        "df.rename(columns={'PAY_AMT1':'PRE_PAY_AMT_SEPT','PAY_AMT2':'PRE_PAY_AMT_AUG','PAY_AMT3':'PRE_PAY_AMT_JUL',\n",
        "                   'PAY_AMT4':'PRE_PAY_AMT_JUN','PAY_AMT5':'PRE_PAY_AMT_MAY','PAY_AMT6':'PRE_PAY_AMT_APR'},inplace=True)\n"
      ],
      "metadata": {
        "id": "pP92Gmrnjosl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "yPsaPBpMjrpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's save this data before operating on it.\n",
        "credit_card_df = df.copy()"
      ],
      "metadata": {
        "id": "odGTmGd1j82g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EDA-**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DR3KqFc3bfbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Although the data in our df in all numerical, there are some categorical variables present in our dataset in encoded form.\n",
        "# Exploring our dependent variable.\n",
        "# first lets rename our dependent variable.\n",
        "df.rename(columns={'default payment next month' : 'is_defaulter'}, inplace=True)"
      ],
      "metadata": {
        "id": "YJbSTjE_aTmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the value counts of our DV\n",
        "plt.figure(figsize=(12,7))\n",
        "sns.countplot(data=df, x='is_defaulter', hue='is_defaulter', palette=['dodgerblue', 'salmon'])\n",
        "plt.xlabel('Defaulter status', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.xticks([0, 1], ['Non-defaulters', 'Defaulters'], fontsize=12)\n",
        "plt.legend"
      ],
      "metadata": {
        "id": "pIaN0laCdWc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see from the above graph and value counts, that we have a unbalanced dataset. The no. of instances for class 0 is significantly higher than class 1**"
      ],
      "metadata": {
        "id": "AcxG3pzwhtF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BJ6l-uLhmzel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we have several other categorical columns like marriage, education, sex.\n",
        "# Let's check them and see the relationship with our dependent variable.\n",
        "df['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "cQd7kpfzvnWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['SEX'].value_counts()"
      ],
      "metadata": {
        "id": "kutyCQL40aqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "Ag0qs0-awuAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the education variable, as per our data description, 1 refers to graduate school, 2 refers to university etc.  however we have no understanding of some numbers present. so we will replace these will others**.\n",
        "\n",
        "**Similarly, in our marriage variable, there is a 0 value which has unknown meaning. so we will add that to others.**"
      ],
      "metadata": {
        "id": "9Gfj5iwv03pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As we can see that there are numerical values in these variables. so lets replace them with their original meanings for eda.\n",
        "# we do this by creating another df by copying a slice of current df.\n",
        "\n",
        "cat_var_df = df[['SEX','EDUCATION','MARRIAGE','is_defaulter']].copy()\n",
        "\n",
        "cat_var_df.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'},\n",
        "                   'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'others', 5:'others',6:'others', 0:'others'},\n",
        "                   'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others', 0 : 'others'}, 'is_defaulter' :{1:'defaulter',0:'non-defaulter'}},\n",
        "                   inplace = True )"
      ],
      "metadata": {
        "id": "ny2ngVe-0mBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var_df.head()"
      ],
      "metadata": {
        "id": "NXX5van_2pIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Plotting the value counts of these categorical variables.\n",
        "# Also visualizing the relationship of these variables with our dependent variable using subplots on the above categorical dataframe.\n",
        "\n",
        "for col in cat_var_df.columns[:-1]:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  fig, axes = plt.subplots(ncols=2,figsize=(16,7))\n",
        "\n",
        "  # Plotting the value counts of categorical variables using pie chart.\n",
        "  cat_var_df[col].value_counts().plot(kind=\"pie\",autopct='%1.3f%%',ax = axes[0],subplots=True, legend=True)\n",
        "\n",
        "  # Plotting the relationship between above categorical features and our dependent variables using count plot.\n",
        "  ax = sns.countplot(x=col, data=cat_var_df,  palette = 'coolwarm', hue=\"is_defaulter\" ,edgecolor = 'magenta',lw =3)\n",
        "\n",
        "  # Setting the legend at the best location and setting the title.\n",
        "  plt.legend(loc='best')\n",
        "  plt.title(f'No. of defaulters vs {col}',weight ='bold', fontsize= 15)\n",
        "\n",
        "# Annotating the counts in countplot charts.\n",
        "  for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x()+p.get_width()/2, height+100, '{:1.0f}'.format(height),ha = \"center\", fontsize= 16)"
      ],
      "metadata": {
        "id": "aVrbgfKDJLQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graphs we can see draw following insights:\n",
        "\n",
        "*   There are more females credit card holders, and therefore there are more female defaulters.\n",
        "*   We can clearly see that single people opt for credit cards more than married people.\n",
        "*   We can clearly see that higher educated people tend to opt for credit cards more than other people.\n",
        "\n"
      ],
      "metadata": {
        "id": "PAlEwK9IH7c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the relationship between age and our dependent variable.\n",
        "plt.figure(figsize=(18,6))\n",
        "ax = sns.countplot(x = 'AGE', hue = 'is_defaulter', data =df, lw=2)\n",
        "ax.legend(loc='upper right')\n",
        "plt.title('No. of Defaulter and non-defaulter with age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VCq9ibxpd9Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets explore LIMIT_BAL column which contains the credit limit data of our clients.\n",
        "df['LIMIT_BAL'].describe()"
      ],
      "metadata": {
        "id": "QiLjdXJPnMF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "sns.barplot(x='is_defaulter', y='LIMIT_BAL', data=df)"
      ],
      "metadata": {
        "id": "0ZlQ73_uPScw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the count plot to vizualize the data distribution with respect to Limit Balance\n",
        "plt.figure(figsize=[16, 6])\n",
        "sns.countplot(x='LIMIT_BAL', hue='is_defaulter', data=df, palette='husl')\n",
        "plt.xticks(rotation=90)                       # Rotate the value of x-ticks so values annotated on x-axis don't get squashed together.\n",
        "plt.xlabel('Limit Balance (NT dollar)', fontsize=15)\n",
        "plt.ylabel('Frequency', fontsize=15)\n",
        "plt.title('LIMIT BALANCE ON TYPE OF CREDIT CARD', fontsize=15)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LHC98OQxUik6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Payment Status History**"
      ],
      "metadata": {
        "id": "M17co7lfbckr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the repayment columns for each month.\n",
        "repayment_feature_list = ['REPAY_STATUS_SEPT',\t'REPAY_STATUS_AUG',\t'REPAY_STATUS_JUL',\t'REPAY_STATUS_JUN',\t'REPAY_STATUS_MAY',\t'REPAY_STATUS_APR']\n",
        "\n",
        "# Plotting graph for each payment feature.\n",
        "for pay_column in repayment_feature_list:\n",
        "  plt.figure(figsize=(12,6))\n",
        "  sns.countplot(x = pay_column, hue = 'is_defaulter', data = df ,palette = 'Paired')"
      ],
      "metadata": {
        "id": "kTzAmG1nUtqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above graph it is clear that most often when there is a delay in payment, there is a delay of 2 months. Also we can see that most of our users have revolving credit(value 0) which is defined as credit that is automatically renewed as debts are paid off."
      ],
      "metadata": {
        "id": "qS3iVq9CePgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets now check the bill amount features.\n",
        "# Assigning the bill amount features to a single variable\n",
        "\n",
        "df_bill_amount = df[['BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR']]\n",
        "sns.pairplot(data = df_bill_amount)"
      ],
      "metadata": {
        "id": "Pv-A_J53cZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the correlation between our numerical features.\n",
        "plt.figure(figsize= (20,10))\n",
        "correlation= df.corr()\n",
        "sns.heatmap(correlation, annot=True, cmap='Spectral_r')"
      ],
      "metadata": {
        "id": "PJ3ZI7ZVfFK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detecting outliers in our dataframe**"
      ],
      "metadata": {
        "id": "QEuJhthVgmiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw box plot to see if there is any outliers in our dataset\n",
        "plt.figure (figsize= (18,7))\n",
        "df.boxplot()\n",
        "plt.xticks(rotation=90)\n",
        "# rotating xticks to 90 degrees. this is done when we want our x-axis label annotators to be vertical\n",
        "# because there may not be enough space for us to visualize them."
      ],
      "metadata": {
        "id": "jByvL2RBfs6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplot, we can see that there are quite a few outliers present in our features. And most of these outliers are present in features containing Pre-payment and Bill amount data."
      ],
      "metadata": {
        "id": "VV7D8exkhgER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a list columns in which outliers are present.\n",
        "outlier_columns = ['LIMIT_BAL', 'BILL_AMT_SEPT','BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "                 'BILL_AMT_APR', 'PRE_PAY_AMT_SEPT', 'PRE_PAY_AMT_AUG', 'PRE_PAY_AMT_JUL','PRE_PAY_AMT_JUN', 'PRE_PAY_AMT_MAY',\n",
        "                 'PRE_PAY_AMT_APR']\n",
        "# using IQR method for dropping outliers from above columns\n",
        "Q1 = df[outlier_columns].quantile(0.25)\n",
        "Q3 = df[outlier_columns].quantile(0.75)\n",
        "\n",
        "IQR = Q3 - Q1                   # interquartile range\n",
        "\n",
        "# using interquartile range to find and remove outliers from our dataframe.\n",
        "df = df[~((df[outlier_columns] < (Q1 - 1.5 * IQR)) |(df[outlier_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "metadata": {
        "id": "BfNn58Rih16k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the new shape of the data.\n",
        "df.shape"
      ],
      "metadata": {
        "id": "fBaadxNJixFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping some of the unnecessary columns.\n",
        "df.drop(['ID'], axis=1,inplace =True)"
      ],
      "metadata": {
        "id": "ywmN6IraZsW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "o-jQ9V3xZ7eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering-**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GNY1teN12tvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now checking for correlation among our dependent variables (Multicollinearity) using VIF analysis.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "586cueTLz-Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing VIF analysis\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['is_defaulter']]])"
      ],
      "metadata": {
        "id": "WNnT_xVS2NcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above, that some of our features have high multicollinearity in them particularly the bill amount columns. so we need to do some feature engineering on them."
      ],
      "metadata": {
        "id": "bkx8B_NR3ElC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets add up all bill amount features together in one.\n",
        "df['TOTAL_BILL_PAY'] = df['BILL_AMT_SEPT'] + df['BILL_AMT_AUG'] + df['BILL_AMT_JUL'] + df['BILL_AMT_JUN'] +  df['BILL_AMT_MAY'] + df['BILL_AMT_APR']"
      ],
      "metadata": {
        "id": "ssN6irAg2jQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check again.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['is_defaulter','BILL_AMT_SEPT','BILL_AMT_AUG','BILL_AMT_JUL','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR']]])"
      ],
      "metadata": {
        "id": "QHfL6gbX4E3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Label and One Hot Encoding-**"
      ],
      "metadata": {
        "id": "-XeyW3EQ8ItR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['SEX']"
      ],
      "metadata": {
        "id": "UDEpd34yutR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding. encoding sex variable. assigning 2 to 0 (which means female) and 1 to male\n",
        "df.replace({'SEX' : {1:1,2:0}}, inplace=True)\n",
        "\n",
        "# One hot encoding.\n",
        "df = pd.get_dummies(df,columns=['EDUCATION','MARRIAGE'])"
      ],
      "metadata": {
        "id": "HuEusjGM5jLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5UGrq1lx9Pu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final data shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "OSz3Mu5NGl0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dependent variable and independent variable\n",
        "independent_variables = df.drop(['is_defaulter'],axis=1)\n",
        "dependent_variable = df['is_defaulter']"
      ],
      "metadata": {
        "id": "YLB8d_3hKf8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling the data using zscore.\n",
        "from scipy.stats import zscore\n",
        "x = round(independent_variables.apply(zscore),3)\n",
        "y = dependent_variable"
      ],
      "metadata": {
        "id": "nZxcUQaLK5Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "ZH6BeQ8oJN3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applying SMOTE (Synthetic Minority Oversampling Technique)-**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ma7j1Jj0M7xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have an imbalanced dataset, we are going to need to apply some technique to remedy this. So we will try oversampling technique called SMOTE."
      ],
      "metadata": {
        "id": "fyqDT2iXpf7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying oversampling to overcome class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote= SMOTE()\n",
        "x_train_smote,y_train_smote = smote.fit_resample(x,y)\n",
        "\n",
        "from collections import Counter\n",
        "print('Original dataset shape', Counter(y_train))\n",
        "print('Resample dataset shape', Counter(y_train_smote))\n",
        "Counter(y_train_smote)"
      ],
      "metadata": {
        "id": "hGfg0yvaNc_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MODEL IMPLEMENTATION-**\n"
      ],
      "metadata": {
        "id": "EGbRk9rzmCg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the evaluation metrics that we will need for comparison.\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc, classification_report"
      ],
      "metadata": {
        "id": "IHBYMSSemFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model1. Logistic Regression**"
      ],
      "metadata": {
        "id": "nb2dydGqPIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Logistics Regression and GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "PnxQokedO4to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the model.\n",
        "logistic_model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# define the parameter grid.\n",
        "param_grid = {'penalty':['l1','l2'], 'C' : [0.0001,0.001,0.003,0.004,0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 20, 50, 100] }\n",
        "\n",
        "# implementing the model.\n",
        "logistic_model= GridSearchCV(logistic_model, param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "logistic_model.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "I1GcTmC1PfXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best estimator\n",
        "logistic_model.best_estimator_"
      ],
      "metadata": {
        "id": "tMpdNtWSRN_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the optimal parameters\n",
        "logistic_model.best_params_"
      ],
      "metadata": {
        "id": "ycmgAvINSCO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the predicted probability of target variable.\n",
        "y_train_preds_logistic = logistic_model.predict_proba(x_train_smote)[:,1]\n",
        "y_test_preds_logistic = logistic_model.predict_proba(x_test)[:,1]"
      ],
      "metadata": {
        "id": "SqM8pAnMSfXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the predicted class\n",
        "y_train_class_preds_logistic = logistic_model.predict(x_train_smote)\n",
        "y_test_class_preds_logistic = logistic_model.predict(x_test)"
      ],
      "metadata": {
        "id": "vR3R_hBUTuAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the accuracy on training and unseen test data.\n",
        "logistic_train_accuracy= accuracy_score(y_train_smote, y_train_class_preds_logistic)\n",
        "logistic_test_accuracy= accuracy_score(y_test, y_test_class_preds_logistic)\n",
        "\n",
        "print(\"The accuracy on train data is \", logistic_train_accuracy)\n",
        "print(\"The accuracy on test data is \", logistic_test_accuracy)"
      ],
      "metadata": {
        "id": "e-Cy28oXT9sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing a function for evaluating various metrics\n",
        "def evaluation_metrics(actual, predicted):\n",
        "\n",
        "  \"\"\" This function is used to find the accuracy score , precision score , recall score , f1 score , ROC_AUC Score ,\n",
        "      Confusion Matrix , Classification  report \"\"\"\n",
        "  metrics_list = []\n",
        "  accuracy = accuracy_score(actual,predicted)\n",
        "  precision = precision_score(actual, predicted)\n",
        "  recall = recall_score(actual, predicted)\n",
        "  model_f1_score = f1_score(actual, predicted)\n",
        "  auc_roc_score = roc_auc_score(actual , predicted)\n",
        "  model_confusion_matrix = confusion_matrix(actual , predicted)\n",
        "\n",
        "  metrics_list = [accuracy,precision,recall,model_f1_score,auc_roc_score, model_confusion_matrix]\n",
        "  return metrics_list"
      ],
      "metadata": {
        "id": "powB-hqqUX13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics(y_test, y_test_class_preds_logistic)"
      ],
      "metadata": {
        "id": "GlSimYYEy0Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's store these metrics in a dataframe. that way we can easily compare metrics of different models.\n",
        "# first store this data in a dict.\n",
        "metric_name_list = ['accuracy','precision','recall','f1_score','roc_auc_score','confusion_matrix']\n",
        "metric_values = evaluation_metrics(y_test, y_test_class_preds_logistic)\n",
        "\n",
        "# zipping together above lists to form a dictionary\n",
        "metric_dict = dict(zip(metric_name_list,metric_values))\n",
        "\n",
        "# creating a dataframe out of this.\n",
        "evaluation_metric_df = pd.DataFrame.from_dict(metric_dict, orient='index').reset_index()\n",
        "evaluation_metric_df.columns = ['Evaluation Metric','Logistic Regression']"
      ],
      "metadata": {
        "id": "nzkqkpGSzJLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "DLAMw1w82RG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix from test data\n",
        "\n",
        "labels = ['Non Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_logistic)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, cmap='coolwarm', ax = ax, lw = 3) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('Actual labels')\n",
        "ax.set_title('Confusion Matrix of Logistics Regression from testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# also printing confusion matrix values\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "xaQYNBei8b80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Roc_auc_curve for test data\n",
        "y_test_pred_logistic = logistic_model.predict_proba(x_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,y_test_pred_logistic)\n",
        "plt.plot(fpr,tpr)\n",
        "plt.title(\"Roc_auc_curve on Test data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZHmKQ_JGV19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_logistic)))"
      ],
      "metadata": {
        "id": "ZYapCFFo2SKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "r9PdZBEJZ7Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * We have implemented logistic regression and we are getting accuracy_score is approx 68%\n",
        " * Precision score is around 41% and f1_score is around 50%\n",
        " * roc_auc approx is 67% and recall_score is approx 64%"
      ],
      "metadata": {
        "id": "q3h9v0gbHr1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mode2. Random Forest Classifier**"
      ],
      "metadata": {
        "id": "_wiOUP-AIamk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Random forest\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "qLdFIVfPHp-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf= RandomForestClassifier()                                                              # initializing the model.\n",
        "\n",
        "grid_values = {'n_estimators':[50,80,90,100], 'max_depth':[9,11,14]}              # initializing the parameter grid.\n",
        "grid_rf = GridSearchCV(model_rf, param_grid = grid_values, scoring = 'accuracy', cv=3)\n",
        "\n",
        "# Fitting the model.\n",
        "grid_rf.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "9ucbsDwkHRVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best estimator\n",
        "grid_rf.best_estimator_"
      ],
      "metadata": {
        "id": "WEOnSO4pN5vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "grid_rf.best_params_"
      ],
      "metadata": {
        "id": "AEZ9m43PNyIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the predicted classes\n",
        "y_train_class_preds_rf = grid_rf.predict(x_train_smote)\n",
        "y_test_class_preds_rf = grid_rf.predict(x_test)"
      ],
      "metadata": {
        "id": "QJ8zLW9kN6ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the evaluation metrics using our function and adding it to evaluation dataframe to better read it.\n",
        "evaluation_metric_df['Random Forest']=evaluation_metrics(y_test,y_test_class_preds_rf)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "w1wVA0suRiAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix from test data\n",
        "\n",
        "labels = ['Non Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_rf)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, cmap='coolwarm', ax = ax, lw = 3) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('Actual labels')\n",
        "ax.set_title('Confusion Matrix of Random Forest from testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# also printing confusion matrix values\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "hvexy1rzZRKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_rf)))"
      ],
      "metadata": {
        "id": "7zmVbxrcW0ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Roc_auc_curve from test data\n",
        "\n",
        "y_test_preds_proba_rf = grid_rf.predict_proba(x_test)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_test_preds_proba_rf)\n",
        "auc = roc_auc_score(y_test,  y_test_preds_proba_rf)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on testing data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VCn4jRvzcHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest model has inbuilt support for showing the feature importances - i.e. which feature is more important in coming up with the predicted results. This helps us interpret and understand the model better."
      ],
      "metadata": {
        "id": "7L7KSvFTceTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting columns names from training data\n",
        "features = x_train_smote.columns\n",
        "\n",
        "# getting the feature importances\n",
        "importances = grid_rf.best_estimator_.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "Y0Uwo3iQcd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the feature importances using a horizontal bar graph.\n",
        "plt.figure (figsize= (12,12))\n",
        "plt.title('Relative Feature Importance', fontsize=14)\n",
        "plt.barh(range(len(indices)), importances[indices], color='magenta', edgecolor='mediumblue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vqo463aKc2_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mode3. K-Nearest Neighbour Classifier**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8JFwKP_6b4TH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcxUC-flWnOA"
      },
      "outputs": [],
      "source": [
        "# Import K Nearest Neighbour Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCixNnXsWxoe"
      },
      "outputs": [],
      "source": [
        "# initializing the model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# knn the parameter to be tuned is n_neighbors\n",
        "param_grid = {'n_neighbors':[4,5,6,7,8,10,12,14]}\n",
        "\n",
        "# Fitting the model\n",
        "\n",
        "knn_cv= GridSearchCV(knn,param_grid, scoring = 'accuracy',cv=3)\n",
        "knn_cv.fit(x_train_smote,y_train_smote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t655hSlW372"
      },
      "outputs": [],
      "source": [
        "# find best score\n",
        "knn_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnOgiXa2W5xH"
      },
      "outputs": [],
      "source": [
        "# best parameters\n",
        "knn_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jb6JHBPW-n_"
      },
      "outputs": [],
      "source": [
        "knn_cv.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQqIG36IXFdc"
      },
      "outputs": [],
      "source": [
        "# Get the predicted classes\n",
        "y_train_class_preds_knn = knn_cv.predict(x_train_smote)\n",
        "y_test_class_preds_knn = knn_cv.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the evaluation metrics and adding it to metric dataframe.\n",
        "evaluation_metric_df['KNeighborsClassifier'] = evaluation_metrics(y_test,y_test_class_preds_knn)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "IKKsmiyMj7Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_knn)))"
      ],
      "metadata": {
        "id": "yWnfLFvzmyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix for testing data\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_knn)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, linewidths=1, cmap='coolwarm',ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix of KNN Classifier for testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "gLZbac82kWYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Roc_auc_curve from test data\n",
        "\n",
        "y_test_preds_proba_knn = knn_cv.predict_proba(x_test)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_test_preds_proba_knn)\n",
        "auc = roc_auc_score(y_test,  y_test_preds_proba_rf)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on testing data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OuZ2F_sol0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model4. Support Vector Classifier**"
      ],
      "metadata": {
        "id": "Ot0vygKHosmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing support vector machine algorithm from sklearn\n",
        "from sklearn import svm\n",
        "\n",
        "# initiate a svm Classifier\n",
        "svm_model = svm.SVC(kernel = 'poly',gamma='scale', probability=True)\n",
        "\n",
        "# fit the model using the training sets\n",
        "svm_model.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "n8NupnJusyCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb2TJVOw7B-d"
      },
      "outputs": [],
      "source": [
        "# Get the predicted classes\n",
        "y_train_class_preds_svm = svm_model.predict(x_train_smote)\n",
        "y_test_class_preds_svm = svm_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df['Support Vector classifier'] = evaluation_metrics(y_test,y_test_class_preds_svm)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "sq3UMnvnunFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix for testing data\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_svm)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, linewidths=1, cmap='coolwarm',ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix of SVM Classifier for testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "To_8qOfTxEW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_knn)))"
      ],
      "metadata": {
        "id": "Ylo3hdlXw_1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Roc_auc_curve on taining data\n",
        "\n",
        "y_train_preds_proba_svm = svm_model.predict_proba(x_train_smote)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_train_smote,  y_train_preds_proba_svm )\n",
        "auc = roc_auc_score(y_train_smote,  y_train_preds_proba_svm )\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on Training data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hH64RPUbxoN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, we can compare our models on variour evaluation metric values.\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "9guJe1tu2GBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CwKlBStjx6PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MODEL DEPLOY**\n"
      ],
      "metadata": {
        "id": "VR_fQQk8x70F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "filename = \"defaulter_model.sav\"\n",
        "pickle.dump(svm_model,open(filename,\"wb\"))\n",
        "\n",
        "#loading the model\n",
        "loaded_model = pickle.load(open(\"defaulter_model.sav\",\"rb\"))"
      ],
      "metadata": {
        "id": "kNN1WxuGx_od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit-option-menu"
      ],
      "metadata": {
        "id": "qWoYcmgYavHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Deployment\n",
        "# elasticnet_regressor\n",
        "#pickle library is used to save model\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "#loading the model\n",
        "loaded_model = pickle.load(open(\"C:/Users/lenovo/Desktop/ML_Deploy/defaulter_model.sav\",\"rb\"))\n",
        "\n",
        "\n",
        "#with st.sidebar:\n",
        "\n",
        "#   selected = option_menu(\"defalt_predictor\")\n",
        "\n",
        "\n",
        "def predict_defaulter(input_data):\n",
        "\n",
        "    # changing the input_data to numpy array\n",
        "     input_data_as_numpy_array = np.asarray(input_data)\n",
        "\n",
        "    # reshape the array as we are predicting for one instance\n",
        "     input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "\n",
        "     prediction = loaded_model.predict(input_data_reshaped)\n",
        "\n",
        "     return prediction[0]\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "\n",
        "    # Giving a title\n",
        "    st.title('Defaulter Prediction Web App')\n",
        "\n",
        "    # Getting the input data from the user\n",
        "    LIMIT_BAL = st.text_input('LIMIT_BAL')\n",
        "    SEX = st.text_input('SEX')\n",
        "    AGE = st.text_input('AGE')\n",
        "    REPAY_STATUS_SEPT = st.text_input('REPAY_STATUS_SEPT')\n",
        "    REPAY_STATUS_AUG = st.text_input('REPAY_STATUS_AUG')\n",
        "    REPAY_STATUS_JUL = st.text_input('REPAY_STATUS_JUL')\n",
        "    REPAY_STATUS_JUN = st.text_input('REPAY_STATUS_JUN')\n",
        "    REPAY_STATUS_MAY = st.text_input('REPAY_STATUS_MAY')\n",
        "    REPAY_STATUS_APR = st.text_input('REPAY_STATUS_APR')\n",
        "    BILL_AMT_SEPT = st.text_input('BILL_AMT_SEPT')\n",
        "    BILL_AMT_AUG = st.text_input('BILL_AMT_AUG')\n",
        "    BILL_AMT_JUL = st.text_input('BILL_AMT_JUL')\n",
        "    BILL_AMT_JUN = st.text_input('BILL_AMT_JUN')\n",
        "    BILL_AMT_MAY = st.text_input('BILL_AMT_MAY')\n",
        "    BILL_AMT_APR = st.text_input('BILL_AMT_APR')\n",
        "    PRE_PAY_AMT_SEPT = st.text_input('PRE_PAY_AMT_SEPT')\n",
        "    PRE_PAY_AMT_AUG = st.text_input('PRE_PAY_AMT_AUG')\n",
        "    PRE_PAY_AMT_JUL = st.text_input('PRE_PAY_AMT_JUL')\n",
        "    PRE_PAY_AMT_JUN = st.text_input('PRE_PAY_AMT_JUN')\n",
        "    PRE_PAY_AMT_MAY = st.text_input('PRE_PAY_AMT_MAY')\n",
        "    PRE_PAY_AMT_APR = st.text_input('PRE_PAY_AMT_APR')\n",
        "    TOTAL_BILL_PAY = st.text_input('TOTAL_BILL_PAY')\n",
        "    EDUCATION_0 = st.text_input('EDUCATION_0')\n",
        "    EDUCATION_1 = st.text_input('EDUCATION_1')\n",
        "    EDUCATION_2 = st.text_input('EDUCATION_2')\n",
        "    EDUCATION_3 = st.text_input('EDUCATION_3')\n",
        "    EDUCATION_4 = st.text_input('EDUCATION_4')\n",
        "    EDUCATION_5 = st.text_input('EDUCATION_5')\n",
        "    EDUCATION_6 = st.text_input('EDUCATION_6')\n",
        "    MARRIAGE_0 = st.text_input('MARRIAGE_0')\n",
        "    MARRIAGE_1 = st.text_input('MARRIAGE_1')\n",
        "    MARRIAGE_2 = st.text_input('MARRIAGE_2')\n",
        "    MARRIAGE_3 = st.text_input('MARRIAGE_3')\n",
        "\n",
        "    # Code for prediction\n",
        "    defaulter_result = ''\n",
        "\n",
        "    # Creating a button for prediction\n",
        "    if st.button('Defaulter  Result'):\n",
        "        Defaulter = [LIMIT_BAL, SEX, AGE, REPAY_STATUS_SEPT, REPAY_STATUS_AUG, REPAY_STATUS_JUL, REPAY_STATUS_JUN, REPAY_STATUS_MAY,\n",
        "                       REPAY_STATUS_APR, BILL_AMT_SEPT, BILL_AMT_AUG, BILL_AMT_JUL, BILL_AMT_JUN, BILL_AMT_MAY,BILL_AMT_APR\n",
        "                       ,PRE_PAY_AMT_SEPT, PRE_PAY_AMT_AUG,PRE_PAY_AMT_JUL, PRE_PAY_AMT_JUN, PRE_PAY_AMT_MAY, PRE_PAY_AMT_APR,\n",
        "                       TOTAL_BILL_PAY, EDUCATION_0, EDUCATION_1,EDUCATION_2,EDUCATION_3,EDUCATION_4,EDUCATION_5,EDUCATION_6,MARRIAGE_0,\n",
        "                       MARRIAGE_1,MARRIAGE_2,MARRIAGE_3,]\n",
        "        prediction = predict_defaulter(Defaulter)\n",
        "\n",
        "        if (prediction == 0):\n",
        "          defaulter_result = 'The person is not Defaulter'\n",
        "        else:\n",
        "          defaulter_result = 'The person is Defaulter'\n",
        "\n",
        "\n",
        "\n",
        "    st.success(defaulter_result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "5M0luvIRalkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}